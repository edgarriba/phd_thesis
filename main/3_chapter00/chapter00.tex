\chapter{Geometric Methods for Localization and Visual Odometry}
\label{chap:p1_00}
\vspace{-8mm}
\section{Localization and Visual Odometry}

In this part of the thesis we deal with the task of localizing a vehicle within a scene using vision, which is one of the first levels of knowledge towards visual scene understanding. The task of localization or ego-localization consists of estimating your own position (and usually orientation) with respect to a scene, that is usually given in the form of a map. For this reason, the task of localization is intimately related to the task of mapping (map creation). In order to delve into these concepts we first need to clarify several key concepts. Firstly, it is important to know that in practice the tasks of localization and mapping can be solved as an atomic task---given rise to Simultaneous Localization and Mapping (SLAM)---, or independently, for instance assuming that a map has already been created and we are just interested in our ego-localization with respect to that map. In the context of ground autonomous vehicles, it is typical that the use of SLAM corresponds to a first stage in which a new area has to be mapped, while localization or re-localization is applied when we know a pre-existing map. The process of visual localization can consider information about the correlation between pre-stored visual references of an area (map) and new references to solve the association problem, but it can also use information coming from the relative motion of the agent (vehicle); something known as Visual Odometry. In many cases, both sources of information are combined.

Allow us to clarify that throughout this thesis we will be using the terms localization and Visual Odometry in a relaxed way, but always to refer to the task of estimating the position and orientation of a vehicle. It is also worth providing a proper definition to the concept of ``map''. In the context of this document we consider that a map is a '``conceptual'' description of a region (or set of regions) of the space encoded in a digital and accessible format. We use the word ``conceptual'' to define the type of description of maps because such a description typically admits a wide range of information. There can be metric 3D maps, which contain the 3D geometry of an area in a metrically consistent fashion. It is also common to find topological maps, \ie, representations of areas in terms of their connections but neglecting part or the totality of the geometric information of the scene. Also, the type of features used to represent maps could drastically vary. Some maps encode geometric information as a sparse set of points, while others encode that information as a dense cloud of points. Some more specialized maps encode semantic information of the scene, such as the types of each object, something that we will show in further detail in chapter~\ref{chap:p2_01}.

\section{A Brief History of Localization and Mapping}
\label{sec:history}

Visual Localization and Mapping tasks are defined as the tasks of estimating the trajectory of a moving robot (or vehicle) and to create a model of the environment, \ie a map. This map can be formed by the references used to track the position of the robot or by any other piece of information considered useful to represent the environment, as for instance: dense 3D point clouds, representative objects like lane-marking and buildings, etc. Usually, the map itself only can be created after knowing the set of positions from which the information was taken (i.e., the robot trajectory). Here, the concept of ''position'' should be understood in a general sense. For topological maps, position may refer just to the relationship between a node and its neighbours, while for metrically-precise maps, position would mean the translational and angular values of the camera pose.

This chicken-and-egg problem dates back to the 80's, as described by Durrant-Whyte and Bailey in \cite{SLAM_SURVEY1}. At the IEEE Robotics and Automation Conference of 1986, some researchers were trying to apply estimation methods to stochastic robot mapping and localization problems \cite{SLAM_SURVEY1}. Among the first pioneers we should highlight Peter Cheeseman, Jim Crowley, Durrant-Whyte, Raja Chatila, Oliver Faugeras and Randal Smith, who actively participated in long discussions about the problem of consistent mapping. These discussions situated localization and mapping as fundamental problems within robotics, and showed some of the conceptual and practical problems that needed to be addressed.

After that, the next keystone was a series of works from different researches, such as Crowley, Chatila and Laumond, which made use of Kalman filter algorithms to perform the localization of the robot. However, the most important contribution was the work presented by Randall Smith, Matthew Self and Peter Cheeseman in \cite{Original}, where the authors described a way of managing and estimating spatial relationships with their respective uncertainty. In this initial conception, the map ---also called stochastic graph--- was just a graph with some spatial constraints and the dependencies between estimations. The paper showed the fact that landmarks are correlated with each other due to the shared error in the estimation of the robot trajectory. In this way, it comes up that any consistent solution of the mapping problem would require the estimation of the vehicle pose, and on the other hand, the estimation of the vehicle pose would require the information about landmarks positions. In other words, both problems were intrinsically connected. 

After this, localization and mapping were widely studied by the robotics and the computer vision communities over more than two decades. This phenomenon was mainly occasioned by the natural capabilities of the formulation for integrating robot trajectories, the visual environment of agents and the uncertainty presented in the relations of these elements. At some point several successfully applications arose for robot navigation in indoor and outdoor scenarios \cite{ConsistencyImprovement}, \cite{OutdoorSLAM}. The interest on these problems helped to generate a remarkable amount of knowledge in this area, and made localization and mapping become mature technologies~\cite{VSLAMDC}. 

However, this maturity has to be understood in the context of the original problem, which imposes strong constraints about the environment and the scene. As a consequence the original definitions of these problems were extended to fit in many new cases that arose in more general contexts. This process also produced some new specializations, as for instance Visual Localization, and Visual Localization and Mapping, which consists of solving the SLAM problem with the sole use of cameras \cite{ParallelVSLAM}. The use of these simple sensors allows the development of accurate autonomous systems at the same time that decreases costs and overall energy consumption. 

%% Nos vehicles
One of these extensions consists of developing the technologies of localization and mapping to the context of Ground Autonomous Vehicles. However, the constraints arising here generate a problem that is much more complex than the original one. Autonomous vehicles operate in more general environments and have to consider the influence of external factors, such as other vehicles, pedestrians and abnormal alterations of the traffic (e.g., due to accidents or traffic jams). With the aim of solving these problems, the intelligent vehicles community adopted most of the tools and techniques produced by the robotics and computer vision fields. Trends that were originated within robotics were increasingly influencing new methods that arose in the intelligent vehicles community. Part of this heritage is found in the use of active sensors, such as lidar and radar to perform localization and mapping. Actually, within the intelligent vehicles literature, is very easy to find many successful approaches that make use of those sensors to acquire the desired data. Good examples of this are \cite{Junior}, \cite{RobustVehicleLoc} and \cite{OffRoad}, which describe practical approaches used in international competitions (e.g., the DARPA Grand Challenge, and the European Land Robot Trials), performing the first tests of these ideas in real conditions.

The use of active sensors can simplify the underlying estimation and mapping stages while producing remarkably good results. Such simplification is achieved by shifting part of the complexity from the core of localization and mapping to the acquisition stage, i.e., acquiring dense clouds of 3D points with lasers simplifies the remaining stages. However, developing these approaches solely based on active sensors might be an important drawback, since there are already low-cost alternatives available, as the ones provided by cameras and vision-based algorithms. Cameras are becoming an essential component of modern cars. They are low-cost and are already there for other scene understanding applications, such as pedestrian detection and obstacle avoidance. These reasons led to the proposal of new approaches based on vision, such as the presented in \cite{VastScale}, \cite{RSLAM} and \cite{VisualOdometryStereo}, where different authors show that this is a feasible technology.

\section{On Robust and Efficient Localization}

We have introduced the task of localization as one of the first levels of understanding in the context of GAVs, showing also how this task is intimately associated to the task of mapping. From a scientific point of view, the problem of localization is approachable from many different viewpoints, but in the context of this thesis we set our focus on the specific tasks of improving robustness and computational efficiency of ego-motion methods, a fundamental block for localization.

In the task of visual localization or visual ego-motion estimation there is uncertainty involved; the data used to perform the process of estimation is noisy and this may lead to the computation of wrong solutions and even produce the instability of the whole system. Thus, the proposal and development of new robust estimators and outlier detection techniques become critical for the reliability of the system. Furthermore, these solutions need to be computationally efficient, since they would have to run on embedded devices on-board of autonomous cars. Therefore, the challenge we address in this thesis is not just to build robust ego-pose estimator, but instead to create suitable approaches that fulfil with the balance between reliability and efficiency.

Taking these principles as our goals, we show how to address the problem of ego-motion estimation by using data embedding approaches and compressed regression methods to achieve extremely efficient solutions (chapter~\ref{chap:p1_01}). We also show how it is possible to operate in compressed regression spaces without neglecting the constraints imposed  by the localization problem (in terms of pose and motion) and maintaining a high level of robustness (chapter~\ref{chap:p1_02}). In fact, an important part of our effort has been dedicated to exploit the constraints that arise when one deals with poses and motion. In this regard, through this first part of the document, we present several approaches to deal with these constraints by performing optimization on Lie-groups and Riemannian manifolds (chapters~\ref{chap:p1_03} and~\ref{chap:p1_04}). Overall, the leitmotiv of the first part of this thesis is to explore techniques to improve estimation robustness that are more efficient than the state-of-the-art, always setting localization for GAVs as our target application.
