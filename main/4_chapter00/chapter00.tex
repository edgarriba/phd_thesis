\graphicspath{{./main/4_chapter00/sections/figures/}}

\chapter[Learning Methods for Scene Understanding]{Learning Methods for Scene\\ Understanding}
\label{chap:p2_00}

\section{Understanding Driving Scenes} 
% Move to the general introduction??
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%This part of the dissertation describes the work done on the topic of visual scene understanding for driving scenes. Scene understanding refers to the task (or set of tasks) enabling an agent to gain a full interpretation of a scene through video, still images or other media, leading to a human-like interpretation and inference of general principles, rules, behaviours and arbitrary situations. In other words, scene understanding is the fundamental capacity of creating a helpful interpretation of our environment; a mental model that represents the first step towards smart decision making. This problem definition is so broad that in practice scene understanding can be seen as a field containing several problems instead of as an atomic entity; and this field largely intersects with the field of computer vision.
%
%The knowledge of a scene is hierarchical, \ie, a scene can be described at different levels of abstraction: it can be described according to the geometry of the scene (\eg, a flat region with a whole in the middle), to respect to the objects it contains (\eg pedestrians, buildings, etc.), according to how these objects are placed on the scene (\eg streets, indoor environment, etc.), in terms of the type of scene, in terms to the actions performed by the objects/entities of the scene (\eg a traffic jam, a fight, etc.), among many other levels~\cite{SceneUnderstanding}. These levels bring different information about a scene and can be addressed as different problems, \eg, object detection, place recognition, semantic segmentation, activity recognition, etc.  
%
%The different levels of abstraction in scene understanding and the different problems that they spawn can be also viewed according to their level of complexity. In this way, recognizing the basic elements of a scene would be at the bottom of the tasks to solve in order to fully understand the scene, followed by the knowledge on how to ``parse'' the elements of the scene considering their associated context (\ie, image parsing or semantic segmentation), and then giving an interpretation of what kind of scene is that (place recognition) and how are their elements interacting (activity recognition). It is therefore, logical to establish a level of maturity for the task of scene understanding according to the degree of maturity obtained when addressing the different levels of abstraction. 

This part of the dissertation describes the work done trying to understand ``what'' is on the scene, \ie, parsing driving scenes as a whole to recognize and segment all the present objects along with the different parts of the road infrastructure.  This problem is known as semantic segmentation. Moreover, we extend and adapt the formulation of semantic segmentation to allow for the recognition of structural changes in the scene. 

In chapter~\ref{chap:intro} we defined scene understanding to be composed by different levels of abstraction, what leads to the formulation of several sub-problems, such as object detection and semantic segmentation among others. We set our interest on parsing the elements of the scene using semantic segmentation, due to the need of a complete description of the scene for autonomous cars. First, we present a basic semantic segmentation pipeline following a classical pipeline composed of hand-crafted features, classifiers and a smoothing technique (Conditional Random Fields). We evaluate the performance of our approach and propose new techniques to improve its computational efficiency and its generalization capacity. Given the computational constraints of GAV systems, we propose a new strategy to leverage the computation of semantics to an offline stage, that is later retrieved after a re-localization process. In order to get around the generalization limitation we propose a caveat that consists of adapting pre-trained models on-the-fly to work in new situations.

From there, and in order to keep improving generalization, we resort to deep learning methods. During the last years the level of maturity associated to scene understanding problems has risen, thanks to the adoption of deep learning tools, proving that learning internal representations is a better practice than the use of hand-crafted features. In fact, Deep learning method have started to achieve super-human performance at tasks like image classification~\cite{He15iccv}. One of the most notorious results is the Deep Residual Networks~\cite{HeICCV15Delving}, a deep learning based method that achieved $3.57\%$ of error in the ILSVRC 2015 image classification task, surpassing human capabilities for general image classification. In the context of driving scenes, another deep learning based approach achieved super-human performance at traffic sign recognition~\cite{Houben-IJCNN-2013}, achieving a $99.44\%$ of accuracy. These results are an example of the maturity of image classification and object recognition problems, that have been due in part to the creation of large datasets, such as ImageNet~\cite{imagenet_cvpr09}, Microsoft COCO~\cite{Lin14eccv} and the blooming of deep learning~\cite{NIPS2012_4824}. However, the state of other problems like semantic segmentation are yet far from producing results of this quality. 

One of the main causes for this has to do with the need of massive datasets required by deep learning; datasets that are not available for driving domains. We lack of equivalents to ImageNet and MS COCO. The process of acquiring a driving dataset is extremely challenging, since it requires to prepare a robotic platform equipped with different sensors such as lasers, cameras, IMUs, etc. All of them must be correctly calibrated (intrinsically and extrinsically) in order to know the relationship between different sensors. In addition to the complexity and cost associated to the development of the robotic platform for data acquisition, one needs to wait for suitable weather conditions for data collection. When collecting the dataset it is usually hard to sample corner cases, since these do not happen very often. In the end, it is easy to end up having a collection of redundant data. After the acquisition, data has to be annotated for the different tasks we would like to address. For semantic segmentation and similar pixelwise-output based problems, this task is extremely tedious and expensive. When dealing with a reduced number of classes, \eg 12 categories, usually takes about 45 minutes. In addition to the large amount of time required to perform the annotation process and the high cost, human annotators tend to introduce noise and incoherence in the annotations. These problems become more evident when the level of abstraction of the task grows, \eg when dealing with road curvature, object orientations, etc. Producing accurate ground truth for these tasks is still an open challenge.

One of the main contributions of this second part is to investigate different techniques to deal with data scarcity within semantic segmentation. We deal with data multi-modality, which arises when resorting to the combination of multiple datasets that are designed for different contexts under different constraints. Dealing with this multi-modality requires the design of new training methods. We also propose to deal with data scarcity by using synthetic data. To achieve this we propose SYNTHIA, a simulation environment of driving scenarios that provides precise ground truth for many scene understanding problems automatically. We investigate the use of virtual imagery in combination to domain adaptation techniques to create models capable of operating in real driving scenarios. During the training process we also consider the constraints imposed by driving scenarios and embedded systems, which limit the size and shape of deep learning architectures. In these situations it becomes critical to produce compact models that can run on embedded hardware with limited memory. We show how to cope with this problem by exploiting transfer learning approaches, to produce models that perform like their state-of-the-art counterparts but require a fraction of the original memory.

At the end of this part we show how the tools constructed to perform semantic segmentation can be adapted to solve the task of (semantic) change detection, therefore enabling systems to understand the high-level knowledge behind human-made changes done to the scene. We propose to use this new tool to perform fast and cost-effective map updating.
