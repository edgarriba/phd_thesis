\section{Experimental Evaluation}
 In this section, two different
experiments are conducted to validate our approach. The goal of
the first experiment is evaluating the ability of the proposed
learning scheme to infer the 3D scene structure from a single
image based on predictions from a classifier trained on a general
database.


The overall idea is showing that this helps the final task. In our case the final task is a semantic segmentation one (worldwide) in a non-supervised way. This is already a step compared to common approaches where only qualitative results are shown (double check \cite{vazquez:2014} where they include some weak evaluations).


\subsection{Datasets}
\label{subsect:datasets}
Experiments are conducted on three different datasets of images
acquired using a camera mounted on a moving
platform~(\fig{fig:datasets}). The first dataset is the
Cambridge-driving Labeled Video Database
(CamVid)~\cite{CamVidBBDD:PRL2008}. CamVid is a publicly available
collection of videos captured in the UK from the
perspective of a driving automobile with ground truth labels that
associate each pixel with one of $32$ semantic classes. This
dataset is divided in two scenarios: day and dusk. The former
consists of images acquired at daytime and the latter consists of
images acquired at nightfall exhibiting completely different
lighting conditions as shown in~\subfig{fig:datasets}{a}. Each of
these scenarios is further divided in two subsets: training and
testing. In our experiments, we use all images for testing. The second dataset is the KITTY dataset~\cite{Geiger2013IJRR}.
KITTY is also a publicly available collection of image sequences
recorded while driving around Karlsruhe, Germany. In particular,
we consider urban images containing city, road and residential
scenarios. Therefore, the dataset is particularly challenging
since images exhibit highlights, cars, different road shapes
as shown in~\subfig{fig:datasets}{b}. Ground truth for this
dataset has been generated by manually annotating $323$ images
randomly selected from the complete dataset. Finally, a third dataset is used consisting of images retrieved from google showing particularly challenging scenarios as shown in~\subfig{fig:datasets}{c}.

\subsection{Evaluations}
We evaluate our algorithm using images recorded
with three different 'uncalibrated' cameras (\ie, different image
resolution and unknown camera parameters) for completely different
scenarios (Spain, Germany and United Kingdom) and at different
daytime. 


Quantitative evaluations are provided on $533$ images from the
other two datasets ($124$ images from Camvid-dusk, $171$
images from Camvid-day test set and $300$ images from KITTY)
without changing parameters or specific tuning (\ie, we assume
there is no validation set).

\subsection{Scene Parsin Algorithms}
We consider three different algorithms: Make3D, PhotoPop-up and Darwin. The first two approaches are exising algorithms for recovering the layout of a scene. These algorithms have been trained with generic images showing different types of environments. For testing purposes we do not retrain them. The last algorithm is the framework introduced in~\cite{}. In this case, the classifier has been trained with domain specific images (road images) using a non-overlapping subset from CamVid and KITTI.

The set of experiments reveals that the preprocessing step improves the performance of existing classifiers in random consitions. To validate the proposal we use three the frameowrks shown in~\fig{} and we consider three instances of classifiers: the approach for recovering the 3D scene layout in~\cite{}, make3D and finally we consider a textoonbost classifier as described in~\cite{}. The first two approaches are used without retraining and represent two different conditions. The first one aims at recovering the layout of a road scene and the neural network was originally trained in using road scenes. The second approach is targeted to general images therefore dealing with the specific conditions of real driving situations is more complicated. Finally, we retrain the approach in~\cite{} using images from ... 


\begin{figure}[t!]
\begin{center}
\begin{tabular}{ccc}
\hspace{-0.05cm}\includegraphics[height=4cm]{./sections/figures/datasetB.eps}&
\hspace{-0.05cm}\includegraphics[height=4cm]{./sections/figures/datasetC.eps}&
\hspace{-0.05cm}\includegraphics[height=4cm]{./sections/figures/datasetA.eps}\\
%\hspace{-0.05cm}\includegraphics[height=4cm]{./sections/figures/datasetGoogle.eps}\\
(a)&(b)&(c)\\
\end{tabular}
\end{center}
\caption{Example of images from the three different datasets: a) Camvid day (left) and dusk (right), b) KITTY (Germany), c) Images retrieved from Google. Images have different resolutions and are taken with different cameras at different scenarios and at different daytime. } \label{fig:datasets}
\end{figure}
\begin{figure*}[!t]
	\centering
	%\vspace{-1mm}
	\includegraphics[scale=0.9]{sections/figures/1TP.eps}
	%\vspace{-5mm}
	\caption{Label.}
	\label{fig:1TP}
	%\vspace{+0mm}
\end{figure*}


\begin{figure*}[!t]
	\centering
	%\vspace{-1mm}
	\includegraphics[scale=0.9]{sections/figures/random.eps}
	%\vspace{-5mm}
	\caption{Label.}
	\label{fig:random}
	%\vspace{+0mm}
\end{figure*}


%\singlespacing
%\renewcommand*\arraystretch{0.5}
\begin{table}
\centering
\resizebox{\columnwidth}{!}{%
%\scriptsize
%\tabcolsep=0.06cm
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\cline{2-9}\noalign{\vskip 1pt}
\multicolumn{1}{c|}{ } & \multicolumn{8}{|c|}{CamVid Dataset}\\
\cline{2-9}\noalign{\vskip 1pt}
\multicolumn{1}{c|}{ } & \multicolumn{2}{|c|}{1TP} & \multicolumn{2}{|c|}{6R0} & \multicolumn{2}{|c|}{16E5} & \multicolumn{2}{|c|}{05SV}\\
\cline{2-9}\noalign{\vskip 1pt}
\multicolumn{1}{c|}{ } & \textbf{OP} & \textbf{PC} & \textbf{OP} & \textbf{PC} & \textbf{OP} & \textbf{PC} & \textbf{OP} & \textbf{PC}\\
\hline
Hoiem (MSRC) Original & 0.61 & 0.38 &	\textbf{0.85} & \textbf{0.65} &	0.87 & 0.67 &	0.87 & 0.64\\
\hline
Make3D Original & 0.58 & 0.35 & 0.87 & 0.68 & 0.84 & 0.61 & 0.91 & 0.70\\
\hline
Darwin (KITTI) Original & 0.68 & 0.61 &	0.84 & 0.77 & \textbf{0.93} & \textbf{0.90} & \textbf{0.89} & \textbf{0.81}\\
\hline
CNN (Barcelona) Original & & & & & & & &\\
\hline\hline
Hoiem (MSRC) Adapted & \textbf{0.83} & \textbf{0.65} & 	0.84 & 0.63 &	\textbf{0.89} & \textbf{0.68} & 	0.87 & 0.64\\
\hline
Make3D Adapted & \textbf{0.71} & \textbf{0.51} & 0.87 & 0.68 & \textbf{0.85} & \textbf{0.63} & 0.91 & \textbf{0.71}\\
\hline
Darwin (KITTI) Adapted & \textbf{0.76} & \textbf{0.71} & 0.84 & 0.77 & 0.91 & 0.84 & 0.85 & 0.73\\
\hline
CNN (Barcelona) Adapted & & & & & & & &\\
\hline

\end{tabular}%
}
\caption{Caption}
\label{tab:results1} 
%\vspace{5mm}
\end{table}

\begin{table}
\centering
\resizebox{\columnwidth}{!}{%
%\scriptsize
%\tabcolsep=0.06cm
\begin{tabular}{|c|c|c|}
\cline{2-3}\noalign{\vskip 1pt}
\multicolumn{1}{c|}{ } & \multicolumn{2}{|c|}{KITTI Road Dataset}\\
\cline{2-3}\noalign{\vskip 1pt}
\multicolumn{1}{c|}{ } & \textbf{OP} & \textbf{PC}\\
\hline
Hoiem (MSRC) Original & 0.67 & 0.65 \\
\hline
Make3D Original & 0.62 & 0.57 \\
\hline
Darwin (Camvid) Original & 0.67 & 0.62 \\
\hline
CNN (Barcelona) & x & x \\
\hline\hline
Hoiem (MSRC) Adapted & 0.67 & 0.65 \\
\hline
Make3D Adapted & 0.61 & 0.57 \\
\hline
Darwin (Camvid) Adapted & 0.64 & 0.62 \\
\hline
CNN (Barcelona) Adapted & x & x \\
\hline
\end{tabular}%
}
\caption{Caption}
\label{tab:results2} 
%\vspace{5mm}
\end{table}