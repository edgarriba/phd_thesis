\section{The Proposed Approach}

Our proposal of unsupervised image transformation is designed to run in real-time
on images acquired in new environments, in which the visual
condition might be severely different from those images used
during the training stage of the semantic labelling framework. The
two main criteria guiding the design of our approach are
summarized as follows. First, image transformation must be
transparent for semantic labelling methods and therefore these two
processes must be decoupled. Second, image transformation must be
very efficient, i.e., real-time capable, in order to be useful for
urban semantic labelling. From these key points we propose a
simple but effective pipeline consisting of two stages:
\textbf{(i)} dictionary creation, where a set of representative
images with enough variability are selected from a reference
domain; and \textbf{(ii)} unsupervised image transformation, where a new
image is first reconstructed on the reference dictionary and then the
outcome of this transformation is used to perform color transfer
based on a variation of Reinhard's method~\cite{Reinhard:2001}.
Both stages are detailed in the following subsections.


\subsection{Dictionary creation}
\label{subsect:basisSelection}


%Consider the typical case where a semantic labelling tool is provided as a black box, i.e., the method is already trained and we want to use it as it is, but on a different domain or scenario. When the principal visual conditions of the new scenario, such as global illumination or colors differ from those seen during training, the accuracy of the segmentation tool dramatically decreases.

%In order to improve the accuracy of semantic labelling tools when images come from scenarios with different visual properties (e.g., global illumination and colors), we propose to transfer such visual properties to match the visual distribution of well-known images.
This initial stage creates a visual dictionary $D_\text{ref}$ of
well-known reference images. Novel images will be matched against
this dictionary in order to perform the color correction. The
construction of the dictionary is an offline process carried out
just once.



%This initial stage creates a visual dictionary $D_\text{ref}$ that
%will be later used to match novel images against well-known
%references in order to perform the color correction. The
%construction of the dictionary is carried out offline and just
%once.

The process starts by evaluating the full dataset in terms of
image quality as proposed in~\cite{XueQualityCVPR2013}, thus
filtering out images of inappropriate illumination and selecting a
subset of $N$ images. Each of these $N$ RGB images $I^i_{RGB}$ is
smoothed via a Gaussian filter of width $w=5$ and $\sigma=0.7$ to
produce $\tilde{I}^i_{RGB}$. Then, each $\tilde{I}^i_{RGB}$ is mapped
onto the $Lab$ color space~\cite{Gonzalez:2002} to perform an
equalization of the lightness channel and to create a histogram
$H^i$ of $B=300$ bins ($100$ bins per channel). The purpose of
this preprocessing step is to account for some variations in the
illumination and color of new images during the matching
process against the reference set. We want to avoid as much as
possible that two images representing places with similar objects
(e.g., red buildings, white buildings, road, etc.), end up
incorrectly matched due to changes in illumination.

The remaining is selecting a subset of $K$ representative images
that will serve as our reference. The best reference database
would contain images used during the training stage of the
semantic labelling framework, if those are available. Otherwise,
it is possible to use a different database containing images
presenting similar visual conditions to those in the training
stage (see section~\ref{subsect:datasets}). The subset of $K$
representatives is chosen to maximize the visual variability, in
order to form a diverse and rich visual dictionary. This process
is summarized as follows. First, the obtained histograms
$\{H^i\}_{i=1}^N$ are grouped into $K$ clusters
$\{H_{\text{ref}}^i\}_{i=1}^K$ using K-Means. These clusters are
considered to be a good representation of the different modes of
the original database, codifying different scenarios such as:
urban areas with buildings, highways, green areas, etc. Finally,
the reference dictionary $D_\text{ref} = \{\tilde{I}^i_{RGB}, H^i_\text{ref})
\}_{i=1}^K$ is formed using the histograms corresponding to these
clusters together with their associated smoothed RGB images. Regarding the selection of the number of clusters $K$, our
experiments show that $K \approx 15$ is a good choice for the
sequences under study. This number seems to be enough to represent
the basic modes of an urban environment.



%
%
%The process starts by selecting a subset of $K$ representative
%images $\{I_{\text{ref}}^i\}_{i=1}^K$ from a larger database that
%will serve as our reference. The best reference database would be
%that containing the images used during the training of the
%semantic labelling tool, if those are available. Otherwise it is
%possible to use a different database containing some images
%presenting similar visual conditions to those of the training
%stage (see section~\ref{subsect:datasets}). The subset of $K$
%representatives is chosen to maximize the visual variability, in
%order to form a diverse and rich visual dictionary.

%
%The selection of the representatives
%$\{I_{\text{ref}}^i\}_{i=1}^K$ is performed as follows. First, the
%full database is evaluated in terms of image quality as proposed
%in~\cite{XueQualityCVPR2013}, thus filtering out images of
%inappropriate illumination. Afterwards, each of the $N$ remaining
%RGB images $I_{RGB}$ is smoothed via a Gaussian filter of width
%$w=5$ and $\sigma=0.7$ to produce $\tilde{I}_{RGB}$. Then, each
%$\tilde{I}_{RGB}$ is mapped onto the Lab color
%space~\cite{Gonzalez:2002} to perform an equalization of the
%lightness channel and to create a histogram $H^i$ of $B=300$ bins
%($100$ bins per channel). The purpose of this preprocessing step
%is to account for some variations in the illumination and color of
%new images during the the matching process against the reference
%set. We want to avoid as much as possible that two images
%representing places with similar objects (e.g., red buildings,
%white buildings, road, etc.), end up incorrectly matched due to
%changes in illumination.
%
%The obtained histograms $\{H^i\}_{i=1}^N$ are then grouped into
%$K$ clusters $\{H_{\text{ref}}^i\}_{i=1}^K$ using K-Means. Such
%clusters are considered to be a good representation of the
%different modes of the original database, codifying different
%scenarios such as: urban areas with buildings, highways, green
%areas, etc. The histograms corresponding to these clusters
%together with their associated smoothed RGB images form the
%reference dictionary $D_\text{ref} = \{\tilde{I}^i_{RGB}, H^i)
%\}_{i=1}^K$.
%
%
%Regarding the selection of the number of clusters $K$, our
%experiments show that $K \approx 15$ is a good choice for the
%sequences under study. This number seems to be enough to represent
%the basic modes of an urban environment.

\subsection{Unsupervised Image Transformation}
\label{subsect:imageAdapt}

In this stage, the new incoming images are corrected on-the-fly,
in a fully unsupervised fashion. To this end we make use of the
concepts proposed by Reinhard \etal~\cite{Reinhard:2001} for
global color correction. However, our approach extends
\cite{Reinhard:2001} in order to generalize the matching process
between test images and reference images from one-to-one to a more convenient one-to-K (one-to-many) matching. This
allows us to consider the different modes of urban scenes and
leads to a more accurate transfer and an improved semantic
labelling, as shown in section~\ref{sec:experiments}.

\subsubsection{Reference Image Selection}
\label{subsubsect:selection} Once the reference dictionary is
built, each incoming image $I^j_\text{test}$ needs to be matched
against the closer representative of the reference dictionary
$D_\text{ref}$. To this end we analyse the histogram
$H^j_\text{test}$ of $I^j_\text{test}$ in terms of the histograms of
$D_\text{ref} = \{\tilde{I}^i_{RGB}, H^i_\text{ref}) \}_{i=1}^K$. First,
$H^j_\text{test}$ is computed as described in
sec.~\ref{subsect:basisSelection}. Then, we reconstruct
$H^j_\text{test}$ as a linear combination of the dictionary vectors
$\{H^i\}_{i=1}^K$. This problem is cast as an $\ell_1$-regularized
least squares problem with nonnegativity constraints (RLS-NN) such
as:

\begin{ceqn}
\begin{eqnarray}
\label{eq:nn}
& W^* = \argmin_W \norm{D_H W - H^j_\text{test}}^2_{\ell_2} + \lambda  \sum_{i=1}^K {W_i} &\\
& \textbf{subject to} \quad W_i \geq 0, \quad i=1, \dots, K. &\nonumber
\end{eqnarray}
\end{ceqn}

Here, $D_H \in \mathbb{R}^{B \times K}$ stands for the $K$ histograms of
$D_\text{ref}$ rearranged as a matrix and $W \in \mathbb{R}^{K}$ is the vector of sparse
nonnegative weights for the reconstructed signal $H^j_\text{test}$.
$\lambda$ is a tunable parameter that controls the sparsity of the
solution and, in our experiments, it has been fixed to $\lambda =
0.1$ through cross validation.


The solution of (\ref{eq:nn}) is computed
using~\cite{Koh07l1ls:2007}, which implements a solution for
large-scale $\ell_1$-regularized least squares problems using a
truncated Newton interior-point method~\cite{KKL07}. Truncated
Newton methods have the advantage of converging in a couple of
iterations, while the cost of each iteration remains efficient for
large systems.


Since $W$ is sparse and nonnegative, we can directly interpret
these weights as the relevance of each reference image in the
reconstruction of $\tilde{I}^j_\text{test}$. The most similar
reference image is the one that most contributed to the
reconstruction, denoted as $\tilde{I}^{j*}_{RGB}$, with $j^* = \argmax_j
W_j $. This image is chosen as the designated reference image for
the matching.



Solving this task as an RLS-NN problem is computationally fast,
and has the advantage that the obtained solution is specifically
designed to be a ''simple'' combination of a few relevant terms.
This is specially interesting for dealing with the transference of
images coming from a video stream.

Dealing with videos, involves taking into account temporal
consistency. Given the high correlation between consecutive
frames, if the image $I^j_\text{test}$ is transferred to the
$k-$th reference image, $I^{j+1}_\text{test}$ should most likely
be transferred to the $k-$th reference too. Including this
constraint can be tricky in methods like K-Nearest Neighbors. In
contrast, in our formulation, it is straightforward to extend
(\ref{eq:nn}) into a Temporal Consistent Transference strategy
(TCT) as follows:

\begin{ceqn}
\begin{eqnarray}
\label{eq:nn2}
& W_\text{Temp}^* = \argmin_W \norm{\widehat{D^\mathcal{L}_H} W - \widehat{H}^\mathcal{L}_\text{test}}^2_{\ell_2} + \lambda  \sum_{i=1}^K {W_i} &\\
& \textbf{subject to} \quad x_i \geq 0, \quad i=1, \dots, K. &\nonumber
\end{eqnarray}
\end{ceqn}

The difference with respect to (\ref{eq:nn}) is that
$\widehat{D^\mathcal{L}_H} = [D_H |\dots|D_H]^T \in \mathbb{R}^{B
\mathcal{L} \times K}$, is the dictionary stacked $\mathcal{L}$
times, where $\mathcal{L}$ stands for the length of a chosen
temporal window (in our experiments $\mathcal{L}=9$). Analogously,
$\widehat{H}^\mathcal{L}_\text{test} =
[H^{j-\mathcal{L}/2}_\text{test} |\dots|H^{j}_\text{test} |\dots|
H^{j+\mathcal{L}/2}_\text{test}]^T \in \mathbb{R}^{B \mathcal{L}}$
is the stack of the histograms of $\mathcal{L}$ consecutive
images. In this way, the resulting vector of weights $W \in
\mathbb{R}^K$ already encodes the best assignment for the temporal
window. Moreover, it is relevant to notice that for moderate
values of $\mathcal{L}$, this method remains computationally
efficient.

\subsubsection{Transferring Colors}
\label{subsubsect:preproc}

\begin{figure}[!t]
    \centering
    %\vspace{-1mm}
    \includegraphics[width=1\textwidth]{artifacts.eps}
    %\vspace{-5mm}
    \caption[Example of Reinhard vs our approach in terms of artefacts.]{Reinhard \textbf{(a)} vs Our approach \textbf{(b)} in terms of artefacts.}
    \label{fig:artefacts}
    %\vspace{+0mm}
\end{figure}

The last part of our approach performs the actual transference of
colors between the matched images $[I^j_\text{test} \leftrightarrow
I^i_{\text{ref}} ]$. This is performed by using a linear
transformation in the CIE $Lab$ perceptual color
space~\cite{Gonzalez:2002}, inspired by Reinhard's seminal
work~\cite{Reinhard:2001}. CIE $Lab$ color space minimizes the
correlation between channels for most natural scenes.
In~\cite{Reinhard:2001}, Reinhard~\etal~define a direct color
transfer technique as a linear transformation along each of the
three channels:

\begin{ceqn}
\begin{align}
\label{reinhard}
L'=(L-\mu^L_t)\frac{\sigma^L_t}{\sigma^L_r}+\mu^L_r \nonumber\\
a'=(a-\mu^a_t)\frac{\sigma^a_t}{\sigma^a_r}+\mu^a_r \\
b'=(b-\mu^b_t)\frac{\sigma^b_t}{\sigma^b_r}+\mu^b_r, \nonumber
\end{align}
\end{ceqn}

\noindent where $\mu^i_t, i\in{L,a,b}$ are the channel means and
$\sigma^i_t, i\in{L,a,b}$ are the channel standard deviations,
calculated over all pixels of the image, for the test image
$I^j_\text{test}$. Similarly, $\mu^i_r$ and $\sigma^i_r,
i\in{L,a,b}$ are the same statistics for the reference image
$I_{\text{ref}}^k$.

Reinhard's method has the advantage of being simple and extremely
fast while providing a fairly good transference of the colors.
However, we noticed that for some visual conditions, this approach
introduces color artefacts that end up affecting the semantic
labelling results. An example of these artefacts is shown in
Fig.~\ref{fig:artefacts}\textbf{(a)}, where the sky become
reddish. This phenomenon is an hallucination of the method that
occurs when the $L$-channel of the test image is saturated and the
method attempts to reduce it. This glitch can be easily fixed by
updating (\ref{reinhard}) as follows:

\begin{ceqn}
\begin{align}
\label{reinhard2}
& L' & = \begin{cases} (L-\mu^L_t)\frac{\sigma^L_t}{\sigma^L_r}+\mu^L_r, & \text{ if } \mu^L_r - \mu^L_t > \tau  \\
L, & \text{otherwise} \end{cases} 
\end{align}
\end{ceqn}

\noindent where $\tau$ is a tunable threshold, which in our experiments remains as $\tau = 50$. Finally, the transferred channels $(L', a', b')$ are mapped back
to the RGB color space to produce the adapted image
${I}^j_\text{Trans}$. At this point the resulting image is ready to
be treated by a semantic labelling method. The performance
benefits of including this transference process are presented in
section~\ref{sec:experiments} in terms of quantitative and
qualitative results on different state-of-the-art semantic
labelling frameworks and datasets.
