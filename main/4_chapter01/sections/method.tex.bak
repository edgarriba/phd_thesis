\section{The Proposed Approach}

Our proposal of image adaptation is designed to run in real-time
on images acquired in new environments, in which the visual
condition might be severely different from those images seen
during the training stage of the semantic segmentation tool. The
two main criteria guiding the design of our approach can be
summarized in two. First, image transformation must be transparent
for semantic segmentation methods and therefore these two
processes must be decoupled. Second, image transformation must be
very efficient, i.e., real-time capable, in order to be useful for
urban semantic segmentation. From these key points  we propose a
simple but effective pipeline consisting of two stages:
\textbf{(i)} dictionary creation, where a set of representative
images with enough variability are selected from a reference
domain; and \textbf{(ii)} on-the-fly image adaptation, where a new
image is first reconstructed on the reference basis and then the
outcome of this transformation is used to perform color correction
based on a variation of Reinhard's method~\cite{Reinhard:2001}.
Both stages are detailed in the following subsections.


\subsection{Dictionary creation}
\label{subsect:basisSelection}

%Consider the typical case where a semantic segmentation tool is provided as a black box, i.e., the method is already trained and we want to use it as it is, but on a different domain or scenario. When the principal visual conditions of the new scenario, such as global illumination or colors differ from those seen during training, the accuracy of the segmentation tool dramatically decreases.

%In order to improve the accuracy of semantic segmentation tools when images come from scenarios with different visual properties (e.g., global illumination and colors), we propose to transfer such visual properties to match the visual distribution of well-known images.

This initial stage creates a visual dictionary $D_\text{ref}$ that will be later used to match novel images against well-known references in order to perform the color correction. The construction of the dictionary is carried out offline and just once.

The process starts by selecting a subset of $K$ representative images $\{I_{\text{ref}}^i\}_{i=1}^K$ from a larger database that will serve as our reference. The best reference database would be that containing the images used during the training of the semantic segmentation tool, if those are available. Otherwise it is possible to use a different database containing some images presenting similar visual conditions to those of the training stage (see section~\ref{subsect:datasets}). The subset of $K$ representatives is chosen to maximize the visual variability, in order to form a diverse and rich visual dictionary.

The selection of the representatives $\{I_{\text{ref}}^i\}_{i=1}^K$ is performed as follows. First, the full database is evaluated in terms of image quality as proposed in~\cite{XueQualityCVPR2013}, thus filtering out images of inappropriate illumination. Afterwards, each of the $N$ remaining RGB images $I_{RGB}$ is smoothed via a Gaussian filter of width $w=5$ and $\sigma=0.7$ to produce $\tilde{I}_{RGB}$. Then, each $\tilde{I}_{RGB}$ is mapped onto the Lab color space~\cite{} to perform an equalization of the lightness channel and to create a histogram $H^i$ of $B=300$ bins ($100$ bins per channel). The purpose of this preprocessing step is to account for some variations in the illumination and color of new images during the the matching process against the reference set. We want to avoid as much as possible that two images representing places with similar objects (e.g., red buildings, white buildings, road, etc.), end up incorrectly matched due to changes in illumination.

The obtained histograms $\{H^i\}_{i=1}^N$ are then grouped into $K$ clusters $\{H_{\text{ref}}^i\}_{i=1}^K$ using K-Means. Such clusters are considered to be a good representation of the different modes of the original database, codifying different scenarios such as: urban areas with buildings, highways, green areas, etc. The histograms corresponding to these clusters together with their associated smoothed RGB images form the reference dictionary $D_\text{ref} = \{\tilde{I}^i_{RGB}, H^i) \}_{i=1}^K$.

Regarding the selection of the number of clusters $K$, our experiments show that $K \approx 15$ is a good choice for the sequences under study. This number seems to be enough to represent the basic modes of a urban environment.

\subsection{On-the-Fly Color Transfer}
\label{subsect:imageAdapt}

In this stage, the new incoming images are corrected on-the-fly, in a fully unsupervised fashion. To this end we make use of the concepts proposed by Reinhard \etal~\cite{Reinhard:2001} for global color correction. However, our approach extends \cite{Reinhard:2001} in order to generalize the matching process between test images and reference images from $[1 \leftrightarrow 1]$ to a more convenient $[1 \leftrightarrow K]$ matching. This allows us to consider the different modes of urban scenes and leads to a more accurate transfer and an improved semantic segmentation, as shown in section~\ref{sec:experiments}.

\subsubsection{Reference Image Selection}
\label{subsubsect:selection}

Once the reference dictionary is built, each incoming image $I_\text{test}$ needs to be matched against the closer representative of the reference dictionary $D_\text{ref}$. To this end we analyse the histogram $H_\text{test}$ of $I_\text{test}$ in terms of the histograms of $D_\text{ref} = \{\tilde{I}^i_{RGB}, H^i) \}_{i=1}^K$. First, $H_\text{test}$ is computed as described in sec.~\ref{subsect:basisSelection}. Then, we reconstruct $H_\text{test}$ as a linear combination of the dictionary vectors $\{H^i\}_{i=1}^K$. This problem is cast as an $\ell_1$-regularized least squares problem with nonnegativity constraints (RLS-NN) such as:

\begin{eqnarray}
\label{eq:nn}
& W^* = \argmin_W \norm{D_H W - H_\text{test}}^2_{\ell_2} + \lambda  \sum_{i=1}^n {x_i} &\\
& \textbf{subject to} \quad x_i \geq 0, \quad i=1, \dots, n. &\nonumber
\end{eqnarray}

Here, $D_H \in \mathbb{R}^{B \times K}$ stands for the histograms $D_\text{ref}$ and $W \in \mathbb{R}^{K}$ are the sparse nonnegative weights for the reconstructed signal $H_\text{test}$. $\lambda$ is a tunable parameter that control the sparsity of the solution and in our experiments it has been fixed to $\lambda = 0.1$ through cross validation.

The solution of \ref{eq:nn} is computed by using~\cite{MatlabSolverL1}, which implements a solution for large-scale $\ell_1$-regularized least squares problems via a truncated Newton interior-point method~\cite{KKL07}. Truncated Newton methods have the advantage of converging in a couple of iterations, while the cost of each iteration remains efficient for large systems.

Since $W$ are sparse and nonnegative, we can directly interpret them as the importance of each reference image in the reconstruction of $\tilde{I}_\text{test}$. Then, the image that most contributed to the reconstruction is $\tilde{I}^{j*}_{RGB}$, with $j^* = \argmax_j W_j $. Therefore, this image is chosen as the designated reference image for the matching, due to their high similarity.

Solving this task as an RLS-NN problem is computationally fast, and has the advantage that the obtained solution is specifically designed to be a ``simple'' combination of a few relevant terms. This results specially interesting for dealing with the transference of images coming from a video stream.

Dealing with videos, involves taking into account temporal consistency, since consecutive images will share a large amount of information. This is a property that can be exploited during the transformation process. It is direct to see that if the image $I^i_\text{test}$ is transferred to the $k-$th reference image, $I^{i+1}_\text{test}$ should most likely be transferred to the $k-$th reference too. Including this constraint can be tricky in methods like K-Nearest Neighbours, but it is straightforward in our formulation, by extending (\ref{eq:nn}) into  a Temporal Consistent Transference strategy (TCT) as shown in (\ref{eq:nn2}):

\begin{eqnarray}
\label{eq:nn2}
& W_\text{Temp}^* = \argmin_W \norm{\widehat{D^\mathcal{L}_H} W - \widehat{H}^\mathcal{L}_\text{test}}^2_{\ell_2} + \lambda  \sum_{i=1}^n {x_i} &\\
& \textbf{subject to} \quad x_i \geq 0, \quad i=1, \dots, n. &\nonumber
\end{eqnarray}

The difference with respect to (\ref{eq:nn}) is that $\widehat{D^\mathcal{L}_H} = [D_H |\dots|D_H]^T \in \mathbb{R}^{B \mathcal{L} \times K}$, is the dictionary stacked $\mathcal{L}$ times, where $L$ stands for the length of a chosen temporal window (in our experiments $\mathcal{L}=9$). Analogously, $\widehat{H}^\mathcal{L}_\text{test} = [H^{i-\mathcal{L}/2}_\text{test} |\dots|H^{i}_\text{test} |\dots| H^{i+\mathcal{L}/2}_\text{test}]^T \in \mathbb{R}^{B \mathcal{L}}$ is the stack of the histograms of $\mathcal{L}$ consecutive images. In this way, the resulting vector of weights $W \in \mathbb{R}^K$ already encodes the best assignment for the temporal window. Moreover, it is relevant to notice that for moderate values of $\mathcal{L}$, this method remains computationally efficient.

\subsubsection{Transferring Colors}
\label{subsubsect:preproc}

The last part of our approach performs the actual transference of colors from the matched images $[I_\text{test} \leftrightarrow I_{\text{ref}}^k ]$. This is performed by using a linear transformation in the CIE $Lab$ perceptual color space~\cite{Gonzalez:2002}, inspired by Reinhard's seminal work~\cite{Reinhard:2001}. CIE $Lab$ color space minimizes the correlation between channels for most natural scenes. Reinhard~\etal defines a direct color transfer technique as a linear transformation along each of the three channels:

\begin{eqnarray}
\label{reinhard}
L'=(L-\mu^L_t)\frac{\sigma^L_t}{\sigma^L_r}+\mu^L_r \nonumber\\
a'=(a-\mu^a_t)\frac{\sigma^a_t}{\sigma^a_r}+\mu^a_r \\
b'=(b-\mu^b_t)\frac{\sigma^b_t}{\sigma^b_r}+\mu^b_r, \nonumber
\end{eqnarray}

\noindent where $\mu^i_t, i\in{L,a,b}$ are the channel means and
$\sigma^i_t, i\in{L,a,b}$ are the channel standard deviations, calculated over all pixels of the image, for the target image. Similarly, $\mu^i_r$ and $\sigma^i_r, i\in{L,a,b}$ are the same statistics for the reference image.

Reinhard's method has the advantage of being simple and extremely fast while providing a fairly good transference of the colors. However, we noticed that for some visual conditions, this approach introduces color artefacts that end up affecting the semantic segmentation results. An example of these artefacts is shown in Fig.~\ref{fig:artifacts}\textbf{(a)}, where the sky become reddish. This phenomenon is an hallucination of the method that occurs when the $L$-channel of the target image is saturated and the method attempts to reduce it. This glitch can be easily fixed by changing (\ref{reinhard}) as follows:

\begin{eqnarray}
\label{reinhard2}
& L' & = \begin{cases} (L-\mu^L_t)\frac{\sigma^L_t}{\sigma^L_r}+\mu^L_r & \text{ if } \mu^L_r - \mu^L_t > \tau  \\ L & \text{otherwise}\end{cases} \nonumber\\
& a' &  =  (a-\mu^a_t)\frac{\sigma^a_t}{\sigma^a_r}+\mu^a_r \\
& b' & = (b-\mu^b_t)\frac{\sigma^b_t}{\sigma^b_r}+\mu^b_r,  \nonumber
\end{eqnarray}

\noindent where $\tau$ is a tunable threshold, which in our experiments remains as $\tau = 50$.

Finally, the transferred channels $(L', a', b')$ are mapped back to the RGB color space to produce the adapted image  ${I}_\text{Trans}$. At this point the resulting image is ready to be treated by a semantic segmentation method, which performance will benefit from the transference process. These benefits are presented in section~\ref{sec:experiments} in terms of quantitative and qualitative results on different state-of-the-art semantic segmentation tools and datasets.

\begin{figure}[!t]
    \centering
    %\vspace{-1mm}
    \includegraphics[scale=0.85]{sections/figures/artifacts.eps}
    %\vspace{-5mm}
    \caption{Reinhard \textbf{(a)} vs Our approach \textbf{(b)} in terms of artefacts.}
    \label{fig:artifacts}
    %\vspace{+0mm}
\end{figure}
